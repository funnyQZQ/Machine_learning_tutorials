### 支持向量机 (Support Vector Machine, SVM)

#### 1. 定义

支持向量机(SVM)是一种强大的监督学习算法,主要用于分类问题。其核心思想是在特征空间中找到一个最优超平面,使得不同类别的样本点到这个超平面的距离最大。

#### 2. 关键概念

##### 2.1 超平面 (Hyperplane)
- 在n维特征空间中分隔数据的n-1维平面
- 可以表示为: $w^Tx + b = 0$
  - 其中 $w$ 是法向量
  - $b$ 是偏置项
  - $x$ 是输入向量

##### 2.2 间隔 (Margin)
- 不同类别的样本点到分隔超平面的最小距离
- SVM的目标是最大化这个间隔
- 数学表达式: $\text{margin} = \frac{2}{||w||}$

##### 2.3 支持向量 (Support Vectors)
- 最接近分隔超平面的训练样本点
- 决定了最终超平面的位置和方向
- 对模型的预测结果有重要影响

##### 2.4 核函数 (Kernel Function)
- 用于将原始特征空间映射到更高维的空间
- 常用核函数包括:
  - 线性核: $K(x_i,x_j) = x_i^T x_j$
  - 多项式核: $K(x_i,x_j) = (x_i^T x_j + c)^d$
  - 高斯核(RBF): $K(x_i,x_j) = \exp(-\gamma ||x_i-x_j||^2)$

##### 2.5 软间隔 (Soft Margin)
- 允许部分样本点被错误分类
- 引入松弛变量 $\xi_i$ 来处理非线性可分的情况
- 优化目标变为: $\min \frac{1}{2}||w||^2 + C\sum_{i=1}^n \xi_i$
  - $C$ 为惩罚参数,控制模型的容错能力


#### 3. 数学推导

##### 3.1 硬间隔SVM推导
对于线性可分的情况,我们的目标是找到最大间隔的分类超平面。

1) 对于任意点 $x_i$ 和其标签 $y_i \in \{-1,+1\}$,我们希望:
   - 当 $y_i=+1$ 时,有 $w^T x_i + b \geq +1$
   - 当 $y_i=-1$ 时,有 $w^T x_i + b \leq -1$
   
   这两个条件可以合并为一个不等式:
   $y_i(w^T x_i + b) \geq 1$
   
   这里的"1"表示我们要求分类边界到超平面的距离至少为 $\frac{1}{||w||}$。这个距离可以通过以下方式推导:
   - 对于任意点 $x$ 到超平面 $w^Tx + b = 0$ 的距离公式为: $\frac{|w^Tx + b|}{||w||}$
   - 根据我们的约束条件 $y_i(w^T x_i + b) \geq 1$
   - 对于支持向量(位于分类边界上的点),等号成立,即 $|w^T x_i + b| = 1$
   - 因此,支持向量到超平面的距离为 $\frac{1}{||w||}$
   这保证了分类间隔的大小为 $\frac{2}{||w||}$ (两侧边界到超平面的距离之和)。

2) 最大化间隔等价于最小化 $\frac{1}{2}||w||^2$,因此优化问题可以写为:

   $$
   \min_{w,b} \frac{1}{2}||w||^2
   $$
   $$
   s.t. \quad y_i(w^T x_i + b) \geq 1, \quad i=1,2,...,n
   $$

3) 使用拉格朗日乘子法:  
   拉格朗日乘子法是一种求解约束优化问题的方法。对于SVM问题:
   - 原始优化目标: $\min \frac{1}{2}||w||^2$
   - 约束条件: $y_i(w^T x_i + b) \geq 1$
   
   引入拉格朗日乘子 $\alpha_i \geq 0$,构造拉格朗日函数:  
   $$L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^n \alpha_i[y_i(w^T x_i + b) - 1]$$
   
   其中:
   - 第一项 $\frac{1}{2}||w||^2$ 是原始的优化目标
   - 第二项是约束条件与拉格朗日乘子的乘积
   - 减号是因为我们的约束是大于等于的形式
   
根据拉格朗日对偶性,原始问题可以转化为对偶形式。对偶性的含义是:
1. 我们可以先对原始变量 $w,b$ 求最小值,再对对偶变量 $\alpha$ 求最大值
2. 也可以先对 $\alpha$ 求最大值,再对 $w,b$ 求最小值 
3. 这两种求解顺序在满足KKT条件时是等价的

因此原始问题可以写为:

$$
\min_{w,b} \max_{\alpha} L(w,b,\alpha) = \max_{\alpha} \min_{w,b} L(w,b,\alpha)
$$

对 $\alpha$ 求最大值的原因:
1. 拉格朗日乘子 $\alpha$ 的作用是惩罚约束条件的违反。当约束条件不满足时(即 $y_i(w^T x_i + b) < 1$),通过增大 $\alpha$,可以使目标函数变大,从而迫使优化过程寻找满足约束的解
2. 从数学角度看,这是将原始的约束优化问题转化为无约束优化问题的必然结果。对偶问题中最大化 $\alpha$ 保证了原始问题的约束条件得到满足
3. 如果约束条件满足,则 $\alpha[y_i(w^T x_i + b) - 1] = 0$,此时最大化 $\alpha$ 不会影响目标函数;如果约束不满足,最大化 $\alpha$ 会使目标函数趋于无穷大,这在优化过程中是不允许的

这个转化基于以下原理:
- 根据拉格朗日对偶性理论,在满足**KKT条件**的情况下,原始问题和对偶问题的最优值相等
- 对于原始问题中的每个不等式约束 $y_i(w^T x_i + b) \geq 1$,我们引入非负的拉格朗日乘子 $\alpha_i \geq 0$
- 通过构造拉格朗日函数 $L(w,b,\alpha)$,我们将约束条件融入目标函数中
- 最小化原始变量 $(w,b)$ 的同时,最大化对偶变量 $\alpha$ 可以保证:
  - 如果原约束满足,则 $\alpha_i[y_i(w^T x_i + b) - 1] = 0$
  - 如果原约束不满足,则最大化会导致目标函数趋于无穷大

通过求解这个问题,我们可以得到原始问题的最优解。

4) 对偶问题求解:
   我们可以通过以下步骤推导出对偶问题:

   1. 首先对拉格朗日函数关于 $w$ 和 $b$ 求偏导并令其为0:
      
      $$
      \frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0
      $$

      得到: $w = \sum_{i=1}^n \alpha_i y_i x_i$
      
      $$
      \frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0 
      $$

      得到: $\sum_{i=1}^n \alpha_i y_i = 0$

   2. 将 $w$ 的表达式代回拉格朗日函数:  
      $$ 
      L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^n \alpha_i[y_i(w^T x_i + b) - 1]
      $$
      
      $$
      = \frac{1}{2}(\sum_{i=1}^n \alpha_i y_i x_i)^T(\sum_{j=1}^n \alpha_j y_j x_j) - \sum_{i=1}^n \alpha_i[y_i((\sum_{j=1}^n \alpha_j y_j x_j)^T x_i + b) - 1]
      $$

   3. 化简后得到对偶问题:

   $$
   \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_i y_j x_i^T x_j
   $$
   $$
   s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \geq 0
   $$


When $a \ne 0$, there are two solutions to $(ax^2 + bx + c = 0)$ and they are 

$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$

The Cauchy-Schwarz Inequality

$$\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)$$