{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机 (Support Vector Machine, SVM)\n",
    "\n",
    "#### 1. 定义\n",
    "\n",
    "支持向量机(SVM)是一种强大的监督学习算法,主要用于分类问题。其核心思想是在特征空间中找到一个最优超平面,使得不同类别的样本点到这个超平面的距离最大。\n",
    "\n",
    "#### 2. 关键概念\n",
    "\n",
    "##### 2.1 超平面 (Hyperplane)\n",
    "- 在n维特征空间中分隔数据的n-1维平面\n",
    "- 可以表示为: $w^Tx + b = 0$\n",
    "  - 其中 $w$ 是法向量\n",
    "  - $b$ 是偏置项\n",
    "  - $x$ 是输入向量\n",
    "\n",
    "##### 2.2 间隔 (Margin)\n",
    "- 不同类别的样本点到分隔超平面的最小距离\n",
    "- SVM的目标是最大化这个间隔\n",
    "- 数学表达式: $\\text{margin} = \\frac{2}{||w||}$\n",
    "\n",
    "##### 2.3 支持向量 (Support Vectors)\n",
    "- 最接近分隔超平面的训练样本点\n",
    "- 决定了最终超平面的位置和方向\n",
    "- 对模型的预测结果有重要影响\n",
    "\n",
    "##### 2.4 核函数 (Kernel Function)\n",
    "- 用于将原始特征空间映射到更高维的空间\n",
    "- 常用核函数包括:\n",
    "  - 线性核: $K(x_i,x_j) = x_i^T x_j$\n",
    "  - 多项式核: $K(x_i,x_j) = (x_i^T x_j + c)^d$\n",
    "  - 高斯核(RBF): $K(x_i,x_j) = \\exp(-\\gamma ||x_i-x_j||^2)$\n",
    "\n",
    "##### 2.5 软间隔 (Soft Margin)\n",
    "- 允许部分样本点被错误分类\n",
    "- 引入松弛变量 $\\xi_i$ 来处理非线性可分的情况\n",
    "- 优化目标变为: $\\min \\frac{1}{2}||w||^2 + C\\sum_{i=1}^n \\xi_i$\n",
    "  - $C$ 为惩罚参数,控制模型的容错能力\n",
    "\n",
    "\n",
    "#### 3. 数学推导\n",
    "\n",
    "##### 3.1 硬间隔SVM推导\n",
    "对于线性可分的情况,我们的目标是找到最大间隔的分类超平面。\n",
    "\n",
    "1) 对于任意点 $x_i$ 和其标签 $y_i \\in \\{-1,+1\\}$,我们希望:\n",
    "   - 当 $y_i=+1$ 时,有 $w^T x_i + b \\geq +1$\n",
    "   - 当 $y_i=-1$ 时,有 $w^T x_i + b \\leq -1$\n",
    "   \n",
    "   这两个条件可以合并为一个不等式:\n",
    "   $y_i(w^T x_i + b) \\geq 1$\n",
    "   \n",
    "   这里的\"1\"表示我们要求分类边界到超平面的距离至少为 $\\frac{1}{||w||}$。这个距离可以通过以下方式推导:\n",
    "   - 对于任意点 $x$ 到超平面 $w^Tx + b = 0$ 的距离公式为: $\\frac{|w^Tx + b|}{||w||}$\n",
    "   - 根据我们的约束条件 $y_i(w^T x_i + b) \\geq 1$\n",
    "   - 对于支持向量(位于分类边界上的点),等号成立,即 $|w^T x_i + b| = 1$\n",
    "   - 因此,支持向量到超平面的距离为 $\\frac{1}{||w||}$\n",
    "   这保证了分类间隔的大小为 $\\frac{2}{||w||}$ (两侧边界到超平面的距离之和)。\n",
    "\n",
    "2) 最大化间隔等价于最小化 $\\frac{1}{2}||w||^2$,因此优化问题可以写为:\n",
    "\n",
    "   $$\n",
    "   \\min_{w,b} \\frac{1}{2}||w||^2\n",
    "   $$\n",
    "   $$\n",
    "   s.t. \\quad y_i(w^T x_i + b) \\geq 1, \\quad i=1,2,...,n\n",
    "   $$\n",
    "\n",
    "3) 使用拉格朗日乘子法:  \n",
    "   拉格朗日乘子法是一种求解约束优化问题的方法。对于SVM问题:\n",
    "   - 原始优化目标: $\\min \\frac{1}{2}||w||^2$\n",
    "   - 约束条件: $y_i(w^T x_i + b) \\geq 1$\n",
    "   \n",
    "   引入拉格朗日乘子 $\\alpha_i \\geq 0$,构造拉格朗日函数:  \n",
    "   $$L(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^n \\alpha_i[y_i(w^T x_i + b) - 1]$$\n",
    "   \n",
    "   其中:\n",
    "   - 第一项 $\\frac{1}{2}||w||^2$ 是原始的优化目标\n",
    "   - 第二项是约束条件与拉格朗日乘子的乘积\n",
    "   - 减号是因为我们的约束是大于等于的形式\n",
    "   \n",
    "根据拉格朗日对偶性,原始问题可以转化为对偶形式。对偶性的含义是:\n",
    "1. 我们可以先对原始变量 $w,b$ 求最小值,再对对偶变量 $\\alpha$ 求最大值\n",
    "2. 也可以先对 $\\alpha$ 求最大值,再对 $w,b$ 求最小值 \n",
    "3. 这两种求解顺序在满足KKT条件时是等价的\n",
    "\n",
    "因此原始问题可以写为:\n",
    "\n",
    "$$\n",
    "\\min_{w,b} \\max_{\\alpha} L(w,b,\\alpha) = \\max_{\\alpha} \\min_{w,b} L(w,b,\\alpha)\n",
    "$$\n",
    "\n",
    "对 $\\alpha$ 求最大值的原因:\n",
    "1. 拉格朗日乘子 $\\alpha$ 的作用是惩罚约束条件的违反。当约束条件不满足时(即 $y_i(w^T x_i + b) < 1$),通过增大 $\\alpha$,可以使目标函数变大,从而迫使优化过程寻找满足约束的解\n",
    "2. 从数学角度看,这是将原始的约束优化问题转化为无约束优化问题的必然结果。对偶问题中最大化 $\\alpha$ 保证了原始问题的约束条件得到满足\n",
    "3. 如果约束条件满足,则 $\\alpha[y_i(w^T x_i + b) - 1] = 0$,此时最大化 $\\alpha$ 不会影响目标函数;如果约束不满足,最大化 $\\alpha$ 会使目标函数趋于无穷大,这在优化过程中是不允许的\n",
    "\n",
    "这个转化基于以下原理:\n",
    "- 根据拉格朗日对偶性理论,在满足**KKT条件**的情况下,原始问题和对偶问题的最优值相等\n",
    "- 对于原始问题中的每个不等式约束 $y_i(w^T x_i + b) \\geq 1$,我们引入非负的拉格朗日乘子 $\\alpha_i \\geq 0$\n",
    "- 通过构造拉格朗日函数 $L(w,b,\\alpha)$,我们将约束条件融入目标函数中\n",
    "- 最小化原始变量 $(w,b)$ 的同时,最大化对偶变量 $\\alpha$ 可以保证:\n",
    "  - 如果原约束满足,则 $\\alpha_i[y_i(w^T x_i + b) - 1] = 0$\n",
    "  - 如果原约束不满足,则最大化会导致目标函数趋于无穷大\n",
    "\n",
    "通过求解这个问题,我们可以得到原始问题的最优解。\n",
    "\n",
    "4) 对偶问题求解:\n",
    "   我们可以通过以下步骤推导出对偶问题:\n",
    "\n",
    "   1. 首先对拉格朗日函数关于 $w$ 和 $b$ 求偏导并令其为0:\n",
    "      \n",
    "      $$\n",
    "      \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0\n",
    "      $$\n",
    "\n",
    "      得到: $w = \\sum_{i=1}^n \\alpha_i y_i x_i$\n",
    "      \n",
    "      $$ \n",
    "      \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^n \\alpha_i y_i = 0 \n",
    "      $$\n",
    "\n",
    "      得到: $\\sum_{i=1}^n \\alpha_i y_i = 0$\n",
    "\n",
    "   2. 将 $w$ 的表达式代回拉格朗日函数:  \n",
    "      $$ \n",
    "      L(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^n \\alpha_i[y_i(w^T x_i + b) - 1]\n",
    "      $$\n",
    "      \n",
    "      $$\n",
    "      = \\frac{1}{2}(\\sum_{i=1}^n \\alpha_i y_i x_i)^T(\\sum_{j=1}^n \\alpha_j y_j x_j) - \\sum_{i=1}^n \\alpha_i[y_i((\\sum_{j=1}^n \\alpha_j y_j x_j)^T x_i + b) - 1]\n",
    "      $$\n",
    "\n",
    "   3. 化简后得到对偶问题:\n",
    "\n",
    "   $$\n",
    "   \\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j y_i y_j x_i^T x_j\n",
    "   $$\n",
    "   $$\n",
    "   s.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "   $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
